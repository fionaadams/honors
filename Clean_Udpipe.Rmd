---
title: "Clean_RunAll"
author: "Fiona Adams"
date: "2/13/2020"
output: html_document
---

```{r} 
library(udpipe)
library(readtext)
library(corpus)
library(plyr)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
```

```{r}
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)

transcripts <- readtext("All_Transcripts/*.docx")

#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts$text[1:2])
x <- data.frame(s)
```

```{r}
'%nin%' <- Negate('%in%')
#take out stopwords (see stopwords() for exact words gone)
x_stopwords <- x %>% 
  mutate(stopword = ifelse(lemma %nin% stopwords(), "NOT.STOPWORD", "STOPWORD"))
```

```{r}
#filter out words that aren't useful, ie. numerals, pronouns, etc.
x_filtered <- x_stopwords %>% 
  mutate(upos = as.character(upos)) %>%
  filter(., grepl('ADJ|ADV|INTJ|NOUN|PROPN|VERB', upos)) %>% #explore: adv / intj inclusion
  subset(., nchar(as.character(lemma)) >= 3) %>%
  filter(., stopword != "STOPWORD")

#pre-filtered data: table with word type count
table(x$upos)

#post-filtered data: table with word type count
table(x_filtered$upos)
```

```{r}
# put words back in sentence form, but without the filtered out words
x_concatenated <- x_filtered %>%
  select(doc_id, sentence_id, token, upos) %>% 
  group_by(sentence_id, doc_id) %>% 
  summarise(text=paste(token,collapse=" ")) %>%
  filter(text != "Yeah")
```

ADD SKIPGRAMS BACK IN AT A LATER DATE, SEE "CLEAN.RMD" IN ARCHIVES FOR THIS WORK
<!-- ```{r} -->
<!-- # clean dataframe by concatenating words that appear together relatively often -->
<!-- # https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ -->

<!-- #Get skipgram probabilities -->
<!-- tidy_skipgrams <- x_concatenated %>% -->
<!--     unnest_tokens(ngram, text, token = "ngrams", n=2) %>% #sets sliding window of 2 -->
<!--     mutate(skipgramID = row_number()) %>% #set row number as the ID -->
<!--     unnest_tokens(word, ngram) -->

<!-- #using counts and proportions, finds skipgram probabilities -->
<!-- skipgram_probs <- tidy_skipgrams %>% -->
<!--     pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>% -->
<!--     mutate(p = n / sum(n)) -->

<!-- #find probability of each word occurring in this corpus -->
<!-- unigram_probs <- x_concatenated %>% -->
<!--     unnest_tokens(word, text) %>% -->
<!--     count(word, sort = TRUE) %>% -->
<!--     mutate(p = n / sum(n)) -->

<!-- #normalize it: how often  -->
<!-- normalized_prob <- skipgram_probs %>% -->
<!--     filter(n > 20) %>% -->
<!--     rename(word1 = item1, word2 = item2) %>% -->
<!--     left_join(unigram_probs %>% -->
<!--                   select(word1 = word, p1 = p), -->
<!--               by = "word1") %>% -->
<!--     left_join(unigram_probs %>% -->
<!--                   select(word2 = word, p2 = p), -->
<!--               by = "word2") %>% -->
<!--     mutate(p_together = p / p1 / p2) -->

<!-- # filters just the places where where item1 != item2 and concatenates these words -->
<!-- normalized_prob <- normalized_prob %>% -->
<!--     dplyr::filter(word1!=word2) %>% -->
<!--     mutate( -->
<!--         concatenate = paste(word1, word2, sep = "_"), -->
<!--         pattern = paste(word1, word2), -->
<!--     ) -->

<!-- x_concatenated <- x_concatenated %>% -->
<!--     mutate( -->
<!--         X = NULL, -->
<!--         text = as.character(text) -->
<!--     ) -->

<!-- #Concatenate all words that fit the normalized probability threshold? -->
<!-- for (i in 1:length(normalized_prob$pattern)) { -->
<!--     x_concatenated$text <- str_replace_all(x_concatenated$text, normalized_prob$pattern[i], normalized_prob$concatenate[i]) -->
<!-- } -->
<!-- ``` -->

```{r}
#get rid of "doc" in doc_id for x_concatenated
for(i in 1:nrow(x_concatenated)){
  x_concatenated[2][i,] <- str_remove(x_concatenated[2][i,], "doc")
}

#add numerical id to transcripts, and get rid of full text inclusion
transcripts$id <- seq.int(nrow(transcripts))
transcript_names <- transcripts %>% select(id, doc_id)

#consistent colnames and data types
colnames(x_concatenated) <- c("sentence_id", "id", "sentence")
x_concatenated <- x_concatenated %>% mutate(id = as.numeric(id))

#join them together
final_sentences <- left_join(x_concatenated, transcript_names, by="id")

#get rid of .docx
for(i in 1:nrow(final_sentences)){
  final_sentences[4][i,] <- str_remove(final_sentences[4][i,], ".docx")
}
```

```{r}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")

transcripts_final <- left_join(final_sentences, names, by="doc_id")
```

```{r}
write.csv(transcripts_final, "udpipe_clean_sentences.csv")
```


