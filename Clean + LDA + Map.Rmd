---
title: "Clean + LDA + Mapping"
author: "Fiona Adams"
date: "1/31/2020"
output: html_document
---

#Packages
```{r echo=FALSE}
library(dplyr)
library(data.table)
require(stopwords)
library(stringr)
require(text2vec)
library(readtext)
library(tidyr)
library(tidytext)
library(widyr)
```

#Read In Data
```{r}
transcripts <- readtext("Transcripts/*.docx")

for(i in 1:nrow(transcripts)){
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " Transcript Final copy.docx")
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " Transcript copy.docx")
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " copy.docx")
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " Interview 1")
}
```

#Split into Sentences, with a doc ID
```{r}
#nest the dataframe so each row is a document with nested text
nested_transcripts <- transcripts %>% nest(transcripts = text)

#function that splits interview into sentences
cleaninterview <- function(interview) {
  interview %>% 
    tolower() %>%
    str_replace_all("ms. ", "miss") %>%
    str_replace_all("st. ", "saint") %>%
    str_split("\\. | \\? | \\! ")
}

#make empty dataframe
interview_out <- rep(0, nrow(transcripts))

#apply cleaninterview function to each row of nested dataframe
for (i in 1:nrow(nested_transcripts)){
  cleandat <- cleaninterview(nested_transcripts[[2]][[i]])
  interview_out[[i]] <- cleandat
}

#turn list into dataframe with id column
interview_out <- rbindlist(interview_out, fill=FALSE, idcol = "id")

#change column name of dataframe
colnames(interview_out)[2] <- "sentences"

#add document id

#get numerical ids for each document and take out sentences
transcripts <- transcripts %>% mutate(id = seq.int(nrow(transcripts))) %>% select(id, doc_id)

#map doc number back to the document name in dat_word using dplyr::left_join
final <- left_join(interview_out, transcripts, by="id") %>% unique()

#add sentence id

#make a table that counts how much each id occurs
count_table <- as.data.frame(table(final$id))

#initialize a list
list_out <- list()

#for the # of documents (# of ids), make a list that has the number of sentences in each doc 
for(i in 1:nrow(count_table)){
  list_out[[length(list_out)+1]] <- seq.int(count_table[2][i,])
}

#make this into a dataframe
list_out <- as.data.frame(unlist(list_out))

#combine this dataframe with final
final <- cbind(final, list_out)

#change colname to sentence id
colnames(final)[4] <- "sentence_id"
```

#Clean Data
```{r}
# fix some formatting \ns left in dataset
final$sentences <- str_replace_all(final$sentences, "\n", " ")

# take out short sentences
final <- final %>% mutate(sentences=as.character(sentences), nchar = nchar(sentences)) %>% filter(nchar>35)
#FUTURE STEP: change this so that it looks for sentences that are JUST stopwords and erases those, instead of taking out sentences of a certain length
#then, take out sentences of a certain length, but a shorter one

#remove unnecessary words (stopwords)

no.stopwords <- function(df){
  '%nin%' <- Negate('%in%')
  wordlist <- unlist(strsplit(df, " "))
  without.stopwords <- wordlist[wordlist %nin% stopwords()]
  str_c(without.stopwords,collapse=' ')  
}

#replace new$sentences[[i]] with no.stopwords(new$sentences[[i]])
for(i in 1:nrow(final)) {
  final$sentences[[i]] <- no.stopwords(final$sentences[[i]])
}

#filter out words that are just one, two, or three characters

for(i in 1:nrow(final)) {
  final$sentences[[i]] <-
paste(str_extract_all(final$sentences, '\\w{4,}')[[i]], collapse=' ')
}

# clean dataframe by concatenating words that appear together relatively often
# https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/

#Get skipgram probabilities
tidy_skipgrams <- final %>%
    unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>% #sets sliding window of 2
    mutate(skipgramID = row_number()) %>% #set row number as the ID
    unnest_tokens(word, ngram)

#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))

#find probability of each word occurring in this corpus
unigram_probs <- final %>%
    unnest_tokens(word, sentences) %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n))

#normalize it: how often 
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

# filters just the places where where item1 != item2 and concatenates these words
normalized_prob <- normalized_prob %>%
    dplyr::filter(word1!=word2) %>%
    mutate(
        concatenate = paste(word1, word2, sep = "_"),
        pattern = paste(word1, word2),
    )

final <- final %>%
    mutate(
        X = NULL,
        id = as.character(id),
        sentences = as.character(sentences)
    )

#How many words to concatenate? Code below only does the first 100 rows. We shouldn't do all of them, but what should be the threshold?
for (i in 1:length(normalized_prob$pattern)) {
    final$sentences <- str_replace_all(final$sentences, normalized_prob$pattern[i], normalized_prob$concatenate[i])
}
```

```{r}
# ex. what are the words most associated with treatment in this dataset? 
normalized_prob %>%
    filter(word1 == "treatment") %>%
    arrange(-p_together)
```

#Perform LDA
```{r echo=FALSE, warning=FALSE, results="hide", message=FALSE}
#can't figure out what i'm missing, so changing the name of the df until I 
new <- final

require(text2vec)

set.seed(397)

tokenlen = (length(new$sentences)/3) * 2

tokens = new$sentences[1:tokenlen] %>% 
  tolower %>%
  word_tokenizer

it = itoken(tokens, ids = new$id[1:tokenlen], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = tokenlen, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

Info on sharing this visualization: https://github.com/cpsievert/LDAvis
```{r message=FALSE}
library(LDAvis)
lda_model$plot()
```

#Extract cluster words
```{r}
words_in_clusters <- lda_model$get_top_words(n = 30, lambda = 0.3)
words_in_clusters <- as.data.frame(words_in_clusters) %>% mutate_all(as.character)
```

#Make clusters only unique words
This doesn't work yet!!!!! Fix this!
```{r}
words_in_clusters[!duplicated(words_in_clusters[1:ncol(words_in_clusters)]),]
```



#Map the clusters
```{r}
check.cluster.i <- function(df, i){
  listofwords <- words_in_clusters[,i]
  pattern <- paste(listofwords, collapse = "|")
  
  detection <- str_detect(df, pattern, negate = FALSE) %>%
    as.data.frame() %>% 
    mutate_all(as.character)
  
  suppressWarnings(str_detect(detection, "TRUE"))
}


number_of_clusters <- 20

for(i in 1:nrow(new)) {
  for(j in 1:number_of_clusters){
  new[[paste0("cluster_",j)]][[i]] <- check.cluster.i(new$sentences[[i]], j)
}
}

new[new=="FALSE"]<-""

for(i in 1:number_of_clusters){
  ifelse(colnames(new)[6:ncol(new)][[i]]==paste("cluster", i, sep="_"), new[6:ncol(new)][[i]] <- str_replace(new[6:ncol(new)][[i]], "TRUE", paste("cluster", i, sep="_")), "breaks")
}

#make a column that shows all of the clusters in each sentence, if any
columns <- colnames(new)[6:ncol(new)]
new$clusters <- apply( new[ , columns ] , 1 , paste , collapse = " " )
```


FOR EDA: line graphs of topics vs sentence number
How to do this????? Need to be able to make this a "timeline" with a count, but! each sentence has only one instance of a cluster if it is within that cluster. 

```{r}
library(ggplot2)
new %>% filter(doc_id=="Anderson_Greg" & cluster_1 == "cluster_1") %>% ggplot(aes(x=sentence_id, y=cluster_1)) + geom_jitter()

new %>% filter(doc_id=="Anderson_Greg" & cluster_2 == "cluster_2") %>% ggplot(aes(x=sentence_id, y=cluster_2)) + geom_jitter()
```


