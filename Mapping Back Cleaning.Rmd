---
title: "Mapping Back Cleaning"
author: "Fiona Adams"
date: "1/16/2020"
output: html_document
---

Slack Leslie!:
Each read 3 interviews, one of each "type of person" (Different people)
Underline/highlight parts that will be recurring themes

Potentially:
Identify sentences within a theme, they should have similar sentence vector representation
put all other sentences in vector representation, find ones that are close

Meantime:
1. define the clusters by the words that are unique to that cluster
2. doc 1 SHOULD BE the document, not the sentence ID! Fix this!
3. supervised deletion of clusters that are unclear
4. Do the mapping on the original interview -- full sentences! map sentence to cluster it has words from

Look at:
What clusters are mapped to places that are valuable for a museum exhibit setting, and how do we keep these/refine these to make them more valuable?

Goal for this document: 
-take top words for each cluster, and map them to sentences

```{r}
library(dplyr)
library(stringr)
```

```{r}
words_in_clusters <- read.csv("words_in_clusters.csv")
words_in_clusters <- words_in_clusters %>% mutate_all(as.character)
new <- read.csv("sentence_level_clean.csv", header = TRUE)
new <- new %>%
    mutate(
        X = NULL,
        ID = as.character(ID),
        sentences = as.character(sentences)
    )
```

Goal:
-For each row in words_in_clusters, check if the top 20-30 words of each cluster (column) are in each row of new$sentences
-Then, make a new column named for the cluster (column) they are from, with a TRUE if that sentence (document) includes a word from the given cluster (column)

```{r}
library(dplyr)
library(stringr)
check.cluster.i <- function(df, i){
  listofwords <- words_in_clusters[,i+1]
  pattern <- paste(listofwords, collapse = "|")
  
  detection <- str_detect(df, pattern, negate = FALSE) %>%
    as.data.frame() %>% 
    mutate_all(as.character)
  
  suppressWarnings(str_detect(detection, "TRUE"))
}


number_of_clusters <- 20

for(i in 1:nrow(new)) {
  for(j in 1:number_of_clusters){
  new[[paste0("cluster_",j)]][[i]] <- check.cluster.i(new$sentences[[i]], j)
}
}

new[new=="FALSE"]<-""

for(i in 1:number_of_clusters){
  ifelse(colnames(new)[4:ncol(new)][[i]]==paste("cluster", i, sep="_"), new[4:ncol(new)][[i]] <- str_replace(new[4:ncol(new)][[i]], "TRUE", paste("cluster", i, sep="_")), "breaks")
}
```

```{r}
#make a column that shows all of the clusters in each sentence, if any
columns <- colnames(new)[4:ncol(new)]
new$clusters <- apply( new[ , columns ] , 1 , paste , collapse = " " )
```

```{r}
new <- read.csv("testnew.csv")
```



Another idea for mapping:
-str_replace_all, replace words stigma -> topic1word1
-then, re-split the interview into sentences and see what words from the different topics are in there
-gives us the ability to see which topics are talked about in which order *within* a sentence
-would need to restrict to the *unique* words, or if the word is in multiple topics, ex. 2 3 and 4, say topic234word
-make graphic/table showing overlap in words between topics

