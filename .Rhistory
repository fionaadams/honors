columns <- colnames(sentences)[11:ncol(sentences)]
sentences$clusters <- apply( sentences[ , columns ] , 1 , paste , collapse = "" )
sentences_clusters <- new %>%
mutate(clusters = str_trim(clusters)) %>% #this isn't trimming the whitespace well? idk
select(id, sentence, clusters)
columns <- colnames(sentences)[11:ncol(sentences)]
sentences$clusters <- apply( sentences[ , columns ] , 1 , paste , collapse = "" )
View(sentences)
new <- read.csv("mapped_data.csv")
words_in_clusters <- read.csv("words_in_clusters.csv")
transcripts <- readtext("All_Transcripts/*.docx")
transcripts_filt <- transcripts %>% filter(doc_id == "Anderson_Greg.docx")
s <- udpipe_annotate(udmodel_english, transcripts_filt$text)
x <- data.frame(s)
sentences <- as.data.frame(x$sentence[!duplicated(x$sentence)])
colnames(sentences) <- "sentences"
library(stringr)
check.cluster.i <- function(df, i){
listofwords <- words_in_clusters[,i]
pattern <- paste(listofwords, collapse = "|")
detection <- str_detect(df, pattern, negate = FALSE) %>%
as.data.frame() %>%
mutate_all(as.character)
suppressWarnings(str_detect(detection, "TRUE"))
}
number_of_clusters <- 20
#wow.. this takes 20 minutes
for(i in 1:nrow(sentences)) {
for(j in 1:number_of_clusters){
sentences[[paste0("cluster_",j)]][[i]] <- check.cluster.i(sentences$sentences[[i]], j)
}
}
sentences[sentences=="FALSE"]<-""
for(i in 1:number_of_clusters){
ifelse(colnames(sentences)[2:ncol(sentences)][[i]]==paste("cluster", i, sep="_"), sentences[2:ncol(sentences)][[i]] <- str_replace(sentences[2:ncol(sentences)][[i]], "TRUE", paste(i)), "breaks")
}
columns <- colnames(sentences)[11:ncol(sentences)]
sentences$clusters <- apply( sentences[ , columns ] , 1 , paste , collapse = "" )
#
# sentences_clusters <- new %>%
#   mutate(clusters = str_trim(clusters)) %>% #this isn't trimming the whitespace well? idk
#   select(id, sentence, clusters)
View(sentences)
new <- read.csv("mapped_data.csv")
words_in_clusters <- read.csv("words_in_clusters.csv")
transcripts <- readtext("All_Transcripts/*.docx")
transcripts_filt <- transcripts %>% filter(doc_id == "Anderson_Greg.docx")
s <- udpipe_annotate(udmodel_english, transcripts_filt$text)
x <- data.frame(s)
sentences <- as.data.frame(x$sentence[!duplicated(x$sentence)])
colnames(sentences) <- "sentences"
library(stringr)
check.cluster.i <- function(df, i){
listofwords <- words_in_clusters[,i]
pattern <- paste(listofwords, collapse = "|")
detection <- str_detect(df, pattern, negate = FALSE) %>%
as.data.frame() %>%
mutate_all(as.character)
suppressWarnings(str_detect(detection, "TRUE"))
}
number_of_clusters <- 20
#wow.. this takes 20 minutes
for(i in 1:nrow(sentences)) {
for(j in 1:number_of_clusters){
sentences[[paste0("cluster_",j)]][[i]] <- check.cluster.i(sentences$sentences[[i]], j)
}
}
sentences[sentences=="FALSE"]<-""
for(i in 1:number_of_clusters){
ifelse(colnames(sentences)[2:ncol(sentences)][[i]]==paste("cluster", i, sep="_"), sentences[2:ncol(sentences)][[i]] <- str_replace(sentences[2:ncol(sentences)][[i]], "TRUE", paste(i)), "breaks")
}
columns <- colnames(sentences)[11:ncol(sentences)]
sentences$clusters <- apply( sentences[ , columns ] , 1 , paste , collapse = "," )
#
# sentences_clusters <- new %>%
#   mutate(clusters = str_trim(clusters)) %>% #this isn't trimming the whitespace well? idk
#   select(id, sentence, clusters)
View(sentences)
str_replace_all(sentences$clusters, ",,", ",")
str_replace_all(sentences$clusters, ",,,,,", ",")
str_replace_all(sentences$clusters, ",,,,", ",")
new <- read.csv("mapped_data.csv")
words_in_clusters <- read.csv("words_in_clusters.csv")
transcripts <- readtext("All_Transcripts/*.docx")
transcripts_filt <- transcripts %>% filter(doc_id == "Anderson_Greg.docx")
s <- udpipe_annotate(udmodel_english, transcripts_filt$text)
x <- data.frame(s)
sentences <- as.data.frame(x$sentence[!duplicated(x$sentence)])
colnames(sentences) <- "sentences"
library(stringr)
check.cluster.i <- function(df, i){
listofwords <- words_in_clusters[,i]
pattern <- paste(listofwords, collapse = "|")
detection <- str_detect(df, pattern, negate = FALSE) %>%
as.data.frame() %>%
mutate_all(as.character)
suppressWarnings(str_detect(detection, "TRUE"))
}
number_of_clusters <- 20
#wow.. this takes 20 minutes
for(i in 1:nrow(sentences)) {
for(j in 1:number_of_clusters){
sentences[[paste0("cluster_",j)]][[i]] <- check.cluster.i(sentences$sentences[[i]], j)
}
}
sentences[sentences=="FALSE"]<-""
for(i in 1:number_of_clusters){
ifelse(colnames(sentences)[2:ncol(sentences)][[i]]==paste("cluster", i, sep="_"), sentences[2:ncol(sentences)][[i]] <- str_replace(sentences[2:ncol(sentences)][[i]], "TRUE", paste(i)), "breaks")
}
columns <- colnames(sentences)[11:ncol(sentences)]
sentences$clusters <- apply( sentences[ , columns ] , 1 , paste , collapse = "" )
#
# sentences_clusters <- new %>%
#   mutate(clusters = str_trim(clusters)) %>% #this isn't trimming the whitespace well? idk
#   select(id, sentence, clusters)
View(sentences)
interviews_together <- sentences_clusters %>%
group_by(id) %>%
summarise(text=paste(sentence, clusters,collapse=" "))
View(interviews_together)
interviews_together <- sentences %>%
group_by(id) %>%
summarise(text=paste(sentence, clusters,collapse=" "))
View(sentences)
new <- read.csv("mapped_data.csv")
words_in_clusters <- read.csv("words_in_clusters.csv")
View(new)
library(stringr)
new <- read.csv("udpipe_clean_sentences.csv")
require(text2vec)
set.seed(397)
tokenlen = (length(new$sentence)/3) * 2
tokens = new$sentence[1:tokenlen] %>%
tolower %>%
word_tokenizer
it = itoken(tokens, ids = new$id[1:tokenlen], progressbar = FALSE)
v = create_vocabulary(it) %>%
prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = tokenlen,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
library(LDAvis)
lda_model$plot()
words_in_clusters <- lda_model$get_top_words(n = 30, lambda = 0.3)
words_in_clusters <- as.data.frame(words_in_clusters) %>%
mutate_all(as.character)
write.csv(words_in_clusters, "words_in_clusters.csv")
check.cluster.i <- function(df, i){
listofwords <- words_in_clusters[,i]
pattern <- paste(listofwords, collapse = "|")
detection <- str_detect(df, pattern, negate = FALSE) %>%
as.data.frame() %>%
mutate_all(as.character)
suppressWarnings(str_detect(detection, "TRUE"))
}
number_of_clusters <- 20
#wow.. this takes 20 minutes
for(i in 1:nrow(new)) {
for(j in 1:number_of_clusters){
new[[paste0("cluster_",j)]][[i]] <- check.cluster.i(new$sentence[[i]], j)
}
}
new[new=="FALSE"]<-NA
for(i in 1:number_of_clusters){
ifelse(colnames(new)[10:ncol(new)][[i]]==paste("cluster", i, sep="_"), new[10:ncol(new)][[i]] <- str_replace(new[10:ncol(new)][[i]], "TRUE", paste(i)), "breaks")
}
View(new)
columns <- colnames(new)[11:ncol(new)]
new$clusters <- apply( new[ , columns ] , 1 , paste , collapse = "" )
View(new)
library(stringr)
new <- read.csv("udpipe_clean_sentences.csv")
require(text2vec)
set.seed(397)
tokenlen = (length(new$sentence)/3) * 2
tokens = new$sentence[1:tokenlen] %>%
tolower %>%
word_tokenizer
it = itoken(tokens, ids = new$id[1:tokenlen], progressbar = FALSE)
v = create_vocabulary(it) %>%
prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = tokenlen,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
library(LDAvis)
lda_model$plot()
words_in_clusters <- lda_model$get_top_words(n = 30, lambda = 0.3)
words_in_clusters <- as.data.frame(words_in_clusters) %>%
mutate_all(as.character)
write.csv(words_in_clusters, "words_in_clusters.csv")
check.cluster.i <- function(df, i){
listofwords <- words_in_clusters[,i]
pattern <- paste(listofwords, collapse = "|")
detection <- str_detect(df, pattern, negate = FALSE) %>%
as.data.frame() %>%
mutate_all(as.character)
suppressWarnings(str_detect(detection, "TRUE"))
}
number_of_clusters <- 20
#wow.. this takes 20 minutes
for(i in 1:nrow(new)) {
for(j in 1:number_of_clusters){
new[[paste0("cluster_",j)]][[i]] <- check.cluster.i(new$sentence[[i]], j)
}
}
new[new=="FALSE"]<- ""
for(i in 1:number_of_clusters){
ifelse(colnames(new)[10:ncol(new)][[i]]==paste("cluster", i, sep="_"), new[10:ncol(new)][[i]] <- str_replace(new[10:ncol(new)][[i]], "TRUE", paste(i)), "breaks")
}
columns <- colnames(new)[11:ncol(new)]
new$clusters <- apply( new[ , columns ] , 1 , paste , collapse = "," )
View(new)
interviews_together <- new %>%
group_by(id) %>%
summarise(text=paste(sentence, clusters,collapse=" "))
View(interviews_together)
interviews_together[,1]
interviews_together[1]
interviews_together$text[1]
library(udpipe)
library(readtext)
library(corpus)
library(plyr)
library(dplyr)
library(stopwords)
library(stringr)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)
transcripts <- readtext("All_Transcripts/*.docx")
#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts$text[1:2])
x <- data.frame(s)
'%nin%' <- Negate('%in%')
#take out stopwords (see stopwords() for exact words gone)
x_stopwords <- x %>%
mutate(stopword = ifelse(lemma %nin% stopwords(), "NOT.STOPWORD", "STOPWORD"))
#filter out words that aren't useful, ie. numerals, pronouns, etc.
x_filtered <- x_stopwords %>%
mutate(upos = as.character(upos)) %>%
filter(., grepl('ADJ|ADV|INTJ|NOUN|PROPN|VERB', upos)) %>% #explore: adv / intj inclusion
subset(., nchar(as.character(lemma)) >= 3) %>%
filter(., stopword != "STOPWORD")
#pre-filtered data: table with word type count
table(x$upos)
#post-filtered data: table with word type count
table(x_filtered$upos)
# put words back in sentence form, but without the filtered out words
x_concatenated <- x_filtered %>%
select(doc_id, sentence_id, token, upos) %>%
group_by(sentence_id, doc_id) %>%
summarise(text=paste(token,collapse=" ")) %>%
filter(text != "Yeah")
#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>% #sets sliding window of 2
mutate(skipgramID = row_number()) %>% #set row number as the ID
unnest_tokens(word, ngram)
library(udpipe)
library(readtext)
library(corpus)
library(plyr)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
i
#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>% #sets sliding window of 2
mutate(skipgramID = row_number()) %>% #set row number as the ID
unnest_tokens(word, ngram)
View(x_concatenated)
# clean dataframe by concatenating words that appear together relatively often
# https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/
#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
unnest_tokens(ngram, text, token = "ngrams", n=2) %>% #sets sliding window of 2
mutate(skipgramID = row_number()) %>% #set row number as the ID
unnest_tokens(word, ngram)
#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
skipgram_probs
tidy_skipgrams
library(udpipe)
library(readtext)
library(corpus)
library(plyr)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
# put words back in sentence form, but without the filtered out words
x_concatenated <- x_filtered %>%
select(doc_id, sentence_id, token, upos) %>%
group_by(sentence_id, doc_id) %>%
summarise(text=paste(token,collapse=" ")) %>%
filter(text != "Yeah")
#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
unnest_tokens(ngram, text, token = "ngrams", n=2) %>% #sets sliding window of 2
mutate(skipgramID = row_number()) %>% #set row number as the ID
unnest_tokens(word, ngram)
#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
#find probability of each word occurring in this corpus
unigram_probs <- x_concatenated %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
#normalize it: how often
normalized_prob <- skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2)
# filters just the places where where item1 != item2 and concatenates these words
normalized_prob <- normalized_prob %>%
dplyr::filter(word1!=word2) %>%
mutate(
concatenate = paste(word1, word2, sep = "_"),
pattern = paste(word1, word2),
)
x_concatenated <- x_concatenated %>%
mutate(
X = NULL,
id = as.character(id),
text = as.character(text)
)
x_concatenated <- x_concatenated %>%
mutate(
X = NULL,
text = as.character(text)
)
View(x_concatenated)
#Concatenate all words that fit the normalized probability threshold?
for (i in 1:length(normalized_prob$pattern)) {
x_concatenated$text <- str_replace_all(x_concatenated$text, normalized_prob$pattern[i], normalized_prob$concatenate[i])
}
normalized_prob
normalized_prob %>%
filter(word1 == "treatment") %>%
arrange(-p_together)
# ex. what are the words most associated with treatment in this dataset?
normalized_prob %>%
filter(word1 == "treatment") %>%
arrange(-p_together)
# ex. what are the words most associated with treatment in this dataset?
normalized_prob %>%
filter(word1 == "harm") %>%
arrange(-p_together)
#get rid of "doc" in doc_id for x_concatenated
for(i in 1:nrow(x_concatenated)){
x_concatenated[2][i,] <- str_remove(x_concatenated[2][i,], "doc")
}
#add numerical id to transcripts, and get rid of full text inclusion
transcripts$id <- seq.int(nrow(transcripts))
transcript_names <- transcripts %>% select(id, doc_id)
#consistent colnames and data types
colnames(x_concatenated) <- c("sentence_id", "id", "sentence")
x_concatenated <- x_concatenated %>% mutate(id = as.numeric(id))
#join them together
final_sentences <- left_join(x_concatenated, transcript_names, by="id")
#get rid of .docx
for(i in 1:nrow(final_sentences)){
final_sentences[4][i,] <- str_remove(final_sentences[4][i,], ".docx")
}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")
transcripts_final <- left_join(final_sentences, names, by="doc_id")
View(transcripts_final)
library(udpipe)
library(readtext)
library(corpus)
library(plyr)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)
transcripts <- readtext("All_Transcripts/*.docx")
#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts$text[1:2])
x <- data.frame(s)
'%nin%' <- Negate('%in%')
#take out stopwords (see stopwords() for exact words gone)
x_stopwords <- x %>%
mutate(stopword = ifelse(lemma %nin% stopwords(), "NOT.STOPWORD", "STOPWORD"))
#filter out words that aren't useful, ie. numerals, pronouns, etc.
x_filtered <- x_stopwords %>%
mutate(upos = as.character(upos)) %>%
filter(., grepl('ADJ|ADV|INTJ|NOUN|PROPN|VERB', upos)) %>% #explore: adv / intj inclusion
subset(., nchar(as.character(lemma)) >= 3) %>%
filter(., stopword != "STOPWORD")
#pre-filtered data: table with word type count
table(x$upos)
#post-filtered data: table with word type count
table(x_filtered$upos)
# put words back in sentence form, but without the filtered out words
x_concatenated <- x_filtered %>%
select(doc_id, sentence_id, token, upos) %>%
group_by(sentence_id, doc_id) %>%
summarise(text=paste(token,collapse=" ")) %>%
filter(text != "Yeah")
#get rid of "doc" in doc_id for x_concatenated
for(i in 1:nrow(x_concatenated)){
x_concatenated[2][i,] <- str_remove(x_concatenated[2][i,], "doc")
}
#add numerical id to transcripts, and get rid of full text inclusion
transcripts$id <- seq.int(nrow(transcripts))
transcript_names <- transcripts %>% select(id, doc_id)
#consistent colnames and data types
colnames(x_concatenated) <- c("sentence_id", "id", "sentence")
x_concatenated <- x_concatenated %>% mutate(id = as.numeric(id))
#join them together
final_sentences <- left_join(x_concatenated, transcript_names, by="id")
#get rid of .docx
for(i in 1:nrow(final_sentences)){
final_sentences[4][i,] <- str_remove(final_sentences[4][i,], ".docx")
}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")
transcripts_final <- left_join(final_sentences, names, by="doc_id")
View(transcripts_final)
transcripts %>% transcripts %>% mutate(nchar = nchar(text))
transcripts <- transcripts %>% mutate(nchar = nchar(text))
View(transcript_names)
View(transcripts_final)
View(transcripts)
quantile[1](1:1297)
quantile(1:1297)$1
quantile(1:1297)$25%
quantile(1:1297)$"25%"
quantile(1:1297)$1
quantile(1:1297)[1]
#write function that determined id, connects id to a nchar
#then, for each sentence within that id, give it a relative quantile value
#case_when
quantile(1:1297)[2]
#write function that determined id, connects id to a nchar
#then, for each sentence within that id, give it a relative quantile value
#case_when
#50% quantile
quantile(1:1297)[3]
#75% quantile
quantile(1:1297)[4]
new <- read.csv("mapped_data.csv")
transcripts <- readtext("All_Transcripts/*.docx")
transcripts <- transcripts %>% mutate(nchar = nchar(text)) %>% mutate(quantile1 = quantile(1:nchar)[2])
#25% quantile
quantile(1:1297)[2]
#50% quantile
quantile(1:1297)[3]
#75% quantile
quantile(1:1297)[4]
#write function that determined id, connects id to a nchar
#then, for each sentence within that id, give it a relative quantile value
#case_when
View(transcript_names)
View(transcripts)
#25% quantile
quantile(1297)[2]
#25% quantile
quantile(1:1297)[2]
for(i in 1:nrow(transcripts)){
nchars = transcripts$nchar[i]
transcripts$quantile1 <- quantile(1:nchars)[2]
}
View(transcript_names)
View(transcripts)
new <- read.csv("mapped_data.csv")
transcripts <- readtext("All_Transcripts/*.docx")
transcripts <- transcripts %>% mutate(nchar = nchar(text))
for(i in 1:nrow(transcripts)){
nchars = transcripts$nchar[i]
transcripts$quantile1[i,] <- quantile(1:nchars)[2]
}
for(i in 1:nrow(transcripts)){
nchars = transcripts$nchar[i]
transcripts$quantile1[i] <- quantile(1:nchars)[2]
}
View(transcripts)
new <- read.csv("mapped_data.csv")
transcripts <- readtext("All_Transcripts/*.docx")
transcripts <- transcripts %>% mutate(nchar = nchar(text))
#yay!!!!
for(i in 1:nrow(transcripts)){
nchars = transcripts$nchar[i]
transcripts$quantile1[i] <- quantile(1:nchars)[2]
transcripts$quantile2[i] <- quantile(1:nchars)[3]
transcripts$quantile3[i] <- quantile(1:nchars)[4]
}
View(transcripts)
