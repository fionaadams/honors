---
title: "Co-occuring Words"
author: "Fiona Adams"
date: "12/19/2019"
output: html_document
---

Typically, word vectors are calculated using neural networks. The approach below, of finding words that occur together in the corpus of Minnesota Opioid Project interviews, uses only counting and linear algebra. This is great because it eliminates the need for pre-trained vectors in a deep learning approach, uses familiar techniques that are relatively easy to understand, and doesn't take too long computationally [@juliasilge]. More reasons to not use word2vec are here: [@multithreaded]

```{r}
library(widyr)
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)

cleanparagraphs <- read.csv("cleanparagraphs.csv")
```

**Skipgram probabilities:** how often we find each word near each other word.
**How to get these probabilities:** Define a fixed-size moving window that centers around each word. What is the probability of seeing *word1* and *word2* in this window?
**Defining the moving window size:** When this window is bigger, the process of counting skipgrams takes longer, obviously. Julia Silge, a well-known data scientist at Stack Overflow, used windows of 8 words, so I decided to start with this. Going forward, I'm looking to take some more sophisticated steps to find the best window to use.
**Concatenate words with high co-occuring probabilities:** If probability of co-occuring is above a certain threshold (how should we determine this threshold??) 

```{r}
#Get skipgram probabilities
tidy_skipgrams <- cleanparagraphs %>%
    unnest_tokens(ngram, paragraphs, token = "ngrams", n=8) %>% #sets sliding window of 8
    mutate(skipgramID = row_number()) %>% #set row number as the ID
    unnest_tokens(word, ngram)

#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))
```

```{r}
## Filters just the places where where item1 != item2 and concatenates these words
skipgram_probs <- skipgram_probs %>%
    dplyr::filter(item1!=item2) %>%
    mutate(
        concatenate = paste(item1, item2, sep = "_"),
        pattern = paste(item1, item2),
    )
```

Find where these words appear together in the corpus, and concatenate them
```{r}
## NEW
cleanparagraphs <- cleanparagraphs %>%
    mutate(
        X = NULL,
        ID = as.character(ID),
        paragraphs = as.character(paragraphs)
    )

#How many words to concatenate? Code below only does the first 10 rows. We shouldn't do all of them, but what should be the threshold?
for (i in 1:10) {
    cleanparagraphs$paragraphs <- str_replace_all(cleanparagraphs$paragraphs, skipgram_probs$pattern[i], skipgram_probs$concatenate[i])
}

## Proof that "like just" instances have been replaced with "like_just"
cleanparagraphs$paragraphs[cleanparagraphs$paragraphs %>% str_detect("like just")]
cleanparagraphs$paragraphs[cleanparagraphs$paragraphs %>% str_detect("like_just")]
```


Bibliography to do:
https://juliasilge.com/blog/tidy-word-vectors/
https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/
