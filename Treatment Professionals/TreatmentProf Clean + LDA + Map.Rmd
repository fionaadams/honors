---
title: "Clean + LDA + Map"
author: "Fiona Adams"
date: "2/24/2020"
output: html_document
---
```{r} 
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
require(text2vec)
library(LDAvis)
```

```{r}
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)

transcripts <- readtext("All_Transcripts/*.docx")
```

```{r}
#get rid of .docx
for(i in 1:nrow(transcripts)){
  transcripts[1][i,] <- str_remove(transcripts[1][i,], ".docx")
}

names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")

transcripts_final <- left_join(transcripts, names, by="doc_id")
```

Make separate dataframes for each "type of person"
```{r}
transcripts_familyexp <- transcripts_final %>% filter(family_experience==TRUE)
transcripts_treatment <- transcripts_final %>% filter(treatment_prof==TRUE)
transcripts_personal <- transcripts_final %>% filter(personal_experience==TRUE)
```

Annotate Interviews by Labeling Parts of Speech, and Separate by Sentences
```{r}
#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts_treatment$text)
x <- data.frame(s)
```

Take out Stopwords
```{r}
'%nin%' <- Negate('%in%')
#take out stopwords (see stopwords() for exact words gone)
x_stopwords <- x %>% 
  mutate(stopword = ifelse(lemma %nin% stopwords(), "NOT.STOPWORD", "STOPWORD"))
```

Filter Out Not Useful Parts of Speech
```{r}
#filter out words that aren't useful, ie. numerals, pronouns, etc.
x_filtered <- x_stopwords %>% 
  mutate(upos = as.character(upos)) %>%
  filter(., grepl('ADJ|ADV|INTJ|NOUN|PROPN|VERB', upos)) %>% #explore: adv / intj inclusion
  subset(., nchar(as.character(lemma)) >= 3) %>%
  filter(., stopword != "STOPWORD")
```

```{r}
# put words back in sentence form, but without the filtered out words
x_concatenated <- x_filtered %>%
  dplyr::select(doc_id, sentence_id, token, upos) %>% 
  group_by(sentence_id, doc_id) %>% 
  summarise(text=paste(token,collapse=" ")) %>%
  filter(text != "Yeah") %>%
  mutate(nchar=nchar(text))

colnames(x_concatenated) <- c("sentence_id", "doc_id", "sentences", "nchar")
```

Concatenate Skipgrams (Normalized)
```{r}
# clean dataframe by concatenating words that appear together relatively often
# https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/

#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
    unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>%  #sets sliding window of 2
    mutate(skipgramID = row_number()) %>% #set row number as the ID
    unnest_tokens(word, ngram)

#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))

#find probability of each word occurring in this corpus
unigram_probs <- x_concatenated %>%
    unnest_tokens(word, sentences) %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n))

#normalize it: how often
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2) %>%
    mutate(pattern = paste(word1, word2, sep=" "), concatenate=paste(word1, word2, sep="_")) %>%
    filter(word1 != word2)

normalized_prob = normalized_prob[!duplicated(normalized_prob$pattern),]

#Concatenate all words that fit the normalized probability threshold, if there are more than 10 together
if (nrow(normalized_prob)>=10){
    for(i in 1:length(normalized_prob$pattern)) {
    x_concatenated$sentences <- str_replace_all(x_concatenated$sentences, normalized_prob$pattern[i], normalized_prob$concatenate[i])
}
} else {
    print ("No words to concatenate")
}
```

Add doc_id (the id with the actual name) to x_concatenated
```{r}
#add doc_id to transcripts
transcripts$id <- seq.int(nrow(transcripts))
transcripts_id <- transcripts %>% mutate(id = paste0("doc",id)) %>% select(-text)

#consistent colnames and data types
colnames(x_concatenated) <- c("sentence_id", "id", "sentence", "nchar")

#join them together
new <- left_join(x_concatenated, transcripts_id, by="id")

new <- new %>% mutate(full_id = paste(id, sentence_id, sep="_"), full_id = as.character(full_id)) 
```

#Perform LDA
```{r echo=FALSE, warning=FALSE, results="hide", message=FALSE}
require(text2vec)

set.seed(397)

tokenlen = length(new$sentence)

tokens = new$sentence[1:tokenlen] %>% 
  tolower %>%
  word_tokenizer

it = itoken(tokens, ids = new$full_id[1:tokenlen], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")


#hyperparameters
lda_model = LDA$new(n_topics = 15, doc_topic_prior = 0.1, topic_word_prior = 0.01)


doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = tokenlen, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

```{r}
#classify each sentence!!!
test <- as.data.frame(doc_topic_distr)
library(data.table)

test <- setDT(test, keep.rownames = TRUE)[]
setnames(test, 1, "full_id")

#join them together
classify <- left_join(new, test, by="full_id")

#this labels the sentence as the topic it's most likely to be from
#fix this so it has a probability threshold (potential solution: make new columns for each cluster/topic, if above write the threshold TRUE if below FALSE--similar to previous mapping)
end = ncol(classify)

classify$results <- colnames(classify[7:end])[max.col(classify[7:end], ties.method = "first")]
classify$top_probs <- apply(classify[7:end], 1, function(x) max(x))
```

```{r}
write.csv(classify, "mapped_data.csv")
```

Mapping to full sentences:
```{r}
x_full <- x %>% 
  select(doc_id, sentence_id, sentence) %>% 
  mutate(full_id = paste(doc_id, sentence_id, sep="_"), full_id = as.character(full_id), sentence_id = as.numeric(sentence_id)) %>% 
  unique()

colnames(x_full) <- c("id", "sentence_id", "full_sentence", "full_id")
```

```{r}
full_new <- left_join(x_full, classify, by=c("full_id", "sentence_id", "id"))

full_new_map <- full_new %>% select(full_id, full_sentence, results, top_probs, doc_id, sentence_id)

write.csv(full_new_map, "mapped_treatmentprof.csv")
```




GET TOP WORDS AND VISUALIZE:

Using LDAvis:
Info on sharing this visualization: https://github.com/cpsievert/LDAvis
```{r message=FALSE}
library(LDAvis)
lda_model$plot()
```

Using top words per cluster (get lambda value by deciding while looking at LDAvis):
```{r}
words_in_clusters <- lda_model$get_top_words(n = 30, lambda = 0.3)
words_in_clusters <- as.data.frame(words_in_clusters) %>% 
  mutate_all(as.character)
```

```{r}
write.csv(words_in_clusters, "personalexp_LDA_words.csv")
```


GET FULL INTERVIEW:

Put back into interview format:
```{r}
full_interviews_together <- full_new_map %>%
  group_by(doc_id) %>%
  summarise(text=paste(full_sentence, results, collapse=" "))
```

```{r}
full_interviews_together$text[1] <- str_replace_all(full_interviews_together$text[1], "AS:", "<br> AS: ")
full_interviews_together$text[1] <- str_replace_all(full_interviews_together$text[1], "LB:", "<br> LB: ")
```

```{r}
writeLines(full_interviews_together$text[1], "new_map.rmd")
```

GET TOP SENTENCES:

```{r}
#top sentences V1
full_new_map %>% 
  filter(results=="V1", top_probs==1) %>%
  select(full_sentence, top_probs, results)

#top sentences V2
full_new_map %>% 
  filter(results=="V2", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V3
full_new_map %>% 
  filter(results=="V3", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V4
full_new_map %>% 
  filter(results=="V4", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V5
full_new_map %>% 
  filter(results=="V5", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V6
full_new_map %>% 
  filter(results=="V6", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V7
full_new_map %>% 
  filter(results=="V7", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V8
full_new_map %>% 
  filter(results=="V8", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V9
full_new_map %>% 
  filter(results=="V9", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V10
full_new_map %>% 
  filter(results=="V10", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V11
full_new_map %>% 
  filter(results=="V11", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V12
full_new_map %>% 
  filter(results=="V12", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V13
full_new_map %>% 
  filter(results=="V13", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V14
full_new_map %>% 
  filter(results=="V14", top_probs==1) %>%
  select(full_sentence, top_probs)

#top sentences V15
full_new_map %>% 
  filter(results=="V15", top_probs==1) %>%
  select(full_sentence, top_probs)
```
