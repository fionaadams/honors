---
title: "Doc2Vec"
author: "Fiona Adams"
date: "11/24/2019"
output: html_document
---

```{r message=FALSE, warning=FALSE}
require(readtext)
library(dplyr)
source("Functions.R")

dat_word <- readtext("/Users/fionaadams/Documents/GitHub/honors/TranscriptTest/*.docx")
```

TRY 1:
```{r}
library(text2vec)
library(data.table)
setDT(dat_word)
setkey(dat_word, doc_id)
set.seed(2016L)
all_ids = dat_word$doc_id
train_ids = sample(all_ids, 2*nrow(dat_word)/3) #2/3 of dataset to train
test_ids = setdiff(all_ids, train_ids) #1/3 of dtataset to test
train = dat_word[J(train_ids)]
test = dat_word[J(test_ids)]

# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(train$text, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$doc_id, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)

train_tokens = train$text %>% 
  prep_fun %>% 
  tok_fun
it_train = itoken(train_tokens, 
                  ids = train$doc_id,
                  # turn off progressbar because it won't look nice in rmd
                  progressbar = FALSE)

vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
```

```{r}
dim(dtm_train) #rows, equal to the number of documents, and columns, equal to the number of unique terms
identical(rownames(dtm_train), train$doc_id) #check and check! rownames are the same as the docids
```

Binomial doesn't work--should we do sentiment analysis on each word? What? Help :(
```{r}
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['text']], 
 #                             family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
```

TRY 2:
```{r message=FALSE, warning=FALSE}
require(quanteda)
require(tm)
require(tidytext)
#Make simple corpus
corpus <- Corpus(VectorSource(dat_word)) 

#Clean simple corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
```

```{r}
#This does not work, not well documented--moving on!
#https://rdrr.io/cran/textTinyR/man/Doc2Vec.html
library(textTinyR)
PATH = system.file("/Users/fionaadams/Documents/GitHub/honors/TranscriptTest/*.docx", "word_vecs.txt", package = "textTinyR")

library(tokenizers)
tok_text = tokenize_words(corpus[[1]])

init = Doc2Vec$new(token_list = tok_text, word_vector_FILE = PATH)
out = init$doc2vec_methods(method = "sum_sqrt")
```

TRY 3:
http://text2vec.org/topic_modeling.html
```{r}
library(stringr)
library(text2vec)
# select 1000 rows for faster running times
dat_train = dat_word[1:14, ]
dat_test = dat_word[15:21, ]
prep_fun = function(x) {
  x %>% 
    # make text lower case
    str_to_lower %>% 
    # remove non-alphanumeric symbols
    str_replace_all("[^[:alpha:]]", " ") %>% 
    # collapse multiple spaces
    str_replace_all("\\s+", " ")
}

dat_train$text = prep_fun(dat_train$text)
it = itoken(dat_train$text, progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 5)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer)

tfidf = TfIdf$new()
lsa = LSA$new(n_topics = 10)

# pipe friendly transformation
doc_embeddings = dtm %>% 
  fit_transform(tfidf) %>% 
  fit_transform(lsa)

dim(doc_embeddings)
```

```{r}
new_data = dat_test
new_doc_embeddings = 
  new_data$text %>% 
  itoken(preprocessor = prep_fun, progressbar = FALSE) %>% 
  create_dtm(vectorizer) %>% 
  # apply exaxtly same scaling wcich was used in train data
  transform(tfidf) %>% 
  # embed into same space as was in train data
  transform(lsa)
dim(new_doc_embeddings)
```

TRY 4:
http://text2vec.org/topic_modeling.html
LATENT DIRICHLET ALLOCATION
```{r}
tokens = dat_word$text[1:14] %>% 
  tolower %>% 
  word_tokenizer
it = itoken(tokens, ids = dat_word$doc_id[1:14], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

```{r}
barplot(doc_topic_distr[1, ], xlab = "topic", 
        ylab = "proportion", ylim = c(0, 1), 
        names.arg = 1:ncol(doc_topic_distr))
```

```{r}
new_dtm = itoken(dat_word$text[15:21], tolower, word_tokenizer, ids = dat_word$doc_id[15:21]) %>% 
  create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
```


```{r}
library(LDAvis)
lda_model$plot()
```

