---
title: "Mapping Back Cleaning"
author: "Fiona Adams"
date: "1/16/2020"
output: html_document
---

Goal for this document: 
-take top words for each cluster, and map them to sentences
-find the synonyms using Julia Silge's methodology seen here: https://juliasilge.com/blog/tidy-word-vectors/
-find full sentences that use these words or the top # of synonyms to those words
```{r}
library(dplyr)
library(stringr)
```

Top words for cluster, mapped:
```{r}
words_in_clusters <- read.csv("words_in_clusters.csv")
words_in_clusters <- words_in_clusters %>% mutate_all(as.character)
new <- read.csv("sentence_level_clean.csv", header = TRUE)
new <- new %>%
    mutate(
        X = NULL,
        ID = as.character(ID),
        sentences = as.character(sentences)
    )
```

Goal:
-For each row in words_in_clusters, check if the top 20-30 words of each cluster (column) are in each row of new$sentences
-Then, make a new column named for the cluster (column) they are from, with a TRUE if that sentence (document) includes a word from the given cluster (column)

```{r}
#This checks the sentences to see if a given cluster is in there
check.cluster.i <- function(df, i){
  listofwords <- words_in_clusters[,i+1]
  pattern <- paste(listofwords, collapse = "|")
  detection <- str_detect(df, pattern, negate = FALSE) %>%
    as.data.frame() %>% 
    mutate_all(as.character)
  suppressWarnings(str_detect(detection, "TRUE"))
}

#future steps: automate this so it changes based on the number of clusters there actually are!! can't be that hard??
for(i in 1:nrow(new)) {
  new$V1[[i]] <- check.cluster.i(new$sentences[[i]], 1)
  new$V2[[i]] <- check.cluster.i(new$sentences[[i]], 2)
  new$V3[[i]] <- check.cluster.i(new$sentences[[i]], 3)
  new$V4[[i]] <- check.cluster.i(new$sentences[[i]], 4)
  new$V5[[i]] <- check.cluster.i(new$sentences[[i]], 5)
  new$V6[[i]] <- check.cluster.i(new$sentences[[i]], 6)
  new$V7[[i]] <- check.cluster.i(new$sentences[[i]], 7)
  new$V8[[i]] <- check.cluster.i(new$sentences[[i]], 8)
  new$V9[[i]] <- check.cluster.i(new$sentences[[i]], 9)
  new$V10[[i]] <- check.cluster.i(new$sentences[[i]], 10)
  new$V11[[i]] <- check.cluster.i(new$sentences[[i]], 11)
  new$V12[[i]] <- check.cluster.i(new$sentences[[i]], 12)
  new$V13[[i]] <- check.cluster.i(new$sentences[[i]], 13)
  new$V14[[i]] <- check.cluster.i(new$sentences[[i]], 14)
  new$V15[[i]] <- check.cluster.i(new$sentences[[i]], 15)
  new$V16[[i]] <- check.cluster.i(new$sentences[[i]], 16)
  new$V17[[i]] <- check.cluster.i(new$sentences[[i]], 17)
  new$V18[[i]] <- check.cluster.i(new$sentences[[i]], 18)
  new$V19[[i]] <- check.cluster.i(new$sentences[[i]], 19)
  new$V20[[i]] <- check.cluster.i(new$sentences[[i]], 20)
}
```

Make a new column indicating which clusters the sentence is part of (if any), ex. V1, V2

Is there a better way to do this?? This does V1, V2, V3 but seems like bad coding
```{r}
new$V1 <- str_replace(new$V1, "TRUE", "V1")
new$V2 <- str_replace(new$V1, "TRUE", "V2")
new$V3 <- str_replace(new$V1, "TRUE", "V3")
new$V4 <- str_replace(new$V1, "TRUE", "V4")
new$V5<- str_replace(new$V1, "TRUE", "V5")

new <- new %>% 
  mutate(clusters = paste(V1, V2, V3, V4, V5, sep=" "))
```

Another idea for mapping:
-str_replace_all, replace words stigma -> topic1word1
-then, re-split the interview into sentences and see what words from the different topics are in there
-gives us the ability to see which topics are talked about in which order *within* a sentence
-would need to restrict to the *unique* words, or if the word is in multiple topics, ex. 2 3 and 4, say topic234word
-make graphic/table showing overlap in words between topics

