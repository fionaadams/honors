---
title: "CleaningandLDA"
author: "Fiona Adams"
date: "12/7/2019"
output: html_document
---

```{r message=FALSE}
require(readtext)
library(dplyr)
library(data.table)
library(lattice)
library(udpipe)
library(stringr)
require(tibble)
library(stopwords)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)

dat_word <- readtext("/Users/fionaadams/Documents/GitHub/honors/TranscriptTest/*.docx")
dat_word <- dat_word %>% mutate(num_id = paste("doc", seq.int(nrow(dat_word)))) #get numerical ids, so easier to find in future
```

```{r}
cleaninterview <- function(interview) {
  interview %>% 
    tolower() %>%
    strsplit("\\n+") %>%
    as.data.frame()
}

changecolname <- function(interview) {
  colnames(interview)[1] <- "paragraphs"
  interview$paragraphs <- as.character(interview$paragraphs)
  interview
}

allparagraphs <- function(interviewfolder){
  interview_out <- rep(0, nrow(dat_word))
  
  for (i in 1:nrow(dat_word)) {
    cleandat <- changecolname(cleaninterview(dat_word$text[i]))
    interview_out[[i]] <- cleandat
  }
  
  paragraphs <- rbindlist(interview_out, fill=TRUE)
  return(paragraphs) #this puts them all into one data table
}

new <- allparagraphs(dat_word)

new <- new %>% 
  rowid_to_column("ID") %>% #make ID column
  mutate(ID = paste("doc", ID), paragraphs=as.character(paragraphs), nchar = nchar(paragraphs)) %>%
  filter(grepl("as:", paragraphs)==FALSE, nchar>20) #take out Amy Sullivan's questions 
```

Clean dataframe by taking out unimportant words
```{r}
no.stopwords <- function(df){
  '%nin%' <- Negate('%in%')
  wordlist <- unlist(strsplit(df, " "))
  without.stopwords <- wordlist[wordlist %nin% stopwords()]
  str_c(without.stopwords,collapse=' ')  
}

#test: show what this does
new$paragraphs[[1]]
no.stopwords(new$paragraphs[[1]])

#replace new$paragraphs[[i]] with no.stopwords(new$paragraphs[[i]])
for(i in 1:nrow(new)) {
  new$paragraphs[[i]] <- no.stopwords(new$paragraphs[[i]])
}
```

Make a .csv file for using in future
```{r}
write.csv(new,'cleanparagraphs.csv')
```

LDA
```{r}
require(text2vec)
tokens = new$paragraphs[1:1000] %>% 
  tolower %>%
  word_tokenizer

it = itoken(tokens, ids = new$ID[1:1000], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

```{r}
barplot(doc_topic_distr[1, ], xlab = "topic", 
        ylab = "proportion", ylim = c(0, 1), 
        names.arg = 1:ncol(doc_topic_distr))
```

```{r}
new_dtm = itoken(new$paragraphs[1000:2116], tolower, word_tokenizer, ids = new$ID[1000:2116]) %>% 
  create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
```

```{r}
library(LDAvis)
lda_model$plot()
```

Different way to LDA
```{r}
require(quanteda)
require(tm)
require(tidytext)
#Make simple corpus
corpus <- Corpus(VectorSource(new$paragraphs)) 

#Clean simple corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)

#Turn into complex corpus
corpus <- corpus(corpus)
```

```{r}
corpusdfm <- quanteda::dfm(corpus, verbose = FALSE)

tidycorpus <- tidy(corpusdfm)

dtm <- tidycorpus %>% cast_dtm(document, term, count)
```

```{r}
library(topicmodels)

# set a seed so that the output of the model is predictable
ap_lda <- LDA(dtm, k = 10, control = list(seed = 1234))
ap_lda

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```

```{r}
library(ggplot2)
library(dplyr)
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```


