select(doc_id, sentence_id, token, upos) %>%
group_by(sentence_id, doc_id) %>%
summarise(text=paste(token,collapse=" ")) %>%
filter(text != "Yeah") %>%
mutate(nchar=nchar(text))
colnames(x_concatenated) <- c("sentence_id", "doc_id", "sentences", "nchar")
# clean dataframe by concatenating words that appear together relatively often
# https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/
#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>%  #sets sliding window of 2
mutate(skipgramID = row_number()) %>% #set row number as the ID
unnest_tokens(word, ngram)
#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
#find probability of each word occurring in this corpus
unigram_probs <- x_concatenated %>%
unnest_tokens(word, sentences) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
#normalize it: how often
normalized_prob <- skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2) %>%
mutate(pattern = paste(word1, word2, sep=" "), concatenate=paste(word1, word2, sep="_")) %>%
filter(word1 != word2)
normalized_prob = normalized_prob[!duplicated(normalized_prob$pattern),]
#Concatenate all words that fit the normalized probability threshold, if there are more than 10 together
if (nrow(normalized_prob)>=10){
for(i in 1:length(normalized_prob$pattern)) {
x_concatenated$sentences <- str_replace_all(x_concatenated$sentences, normalized_prob$pattern[i], normalized_prob$concatenate[i])
}
} else {
print ("No words to concatenate")
}
#add doc_id to transcripts
transcripts$id <- seq.int(nrow(transcripts))
transcripts_id <- transcripts %>% mutate(id = paste0("doc",id)) %>% select(-text)
#consistent colnames and data types
colnames(x_concatenated) <- c("sentence_id", "id", "sentence", "nchar")
#join them together
new <- left_join(x_concatenated, transcripts_id, by="id")
require(text2vec)
set.seed(397)
tokenlen = (length(new$sentence)/3) * 2
tokens = new$sentence[1:tokenlen] %>%
tolower %>%
word_tokenizer
it = itoken(tokens, ids = new$id[1:tokenlen], progressbar = FALSE)
v = create_vocabulary(it) %>%
prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = tokenlen,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
library(LDAvis)
lda_model$plot()
words_in_clusters <- lda_model$get_top_words(n = 30, lambda = 0.3)
words_in_clusters <- as.data.frame(words_in_clusters) %>%
mutate_all(as.character)
write.csv(words_in_clusters, "personalexp_LDA_words.csv")
check.cluster.i <- function(df, i){
listofwords <- words_in_clusters[,i]
pattern <- paste(listofwords, collapse = "|")
detection <- str_detect(df, pattern, negate = FALSE) %>%
as.data.frame() %>%
mutate_all(as.character)
suppressWarnings(str_detect(detection, "TRUE"))
}
number_of_clusters <- 20
#wow.. this takes 20 minutes
for(i in 1:nrow(new)) {
for(j in 1:number_of_clusters){
new[[paste0("cluster_",j)]][[i]] <- check.cluster.i(new$sentence[[i]], j)
}
}
new[new=="FALSE"]<- NA
for(i in 1:number_of_clusters){
ifelse(colnames(new)[6:ncol(new)][[i]]==paste("cluster", i, sep="_"), new[6:ncol(new)][[i]] <- str_replace(new[6:ncol(new)][[i]], "TRUE", paste(i)), "breaks")
}
columns <- colnames(new)[10:ncol(new)]
#concatenate cluster values without NAs
new <- new %>% mutate(clusters = "")
new$clusters <- apply(new[, columns], 1, function(x) toString(na.omit(x)))
# new <- new %>% mutate(ifelse(clusters == "", "", paste0(" (", clusters, ")")))
new <- new %>% mutate(clusters = ifelse(clusters == "", "", paste0("<span style=\"color:red\">", clusters, "</span>")))
x_full <- x %>% select(doc_id, sentence_id, sentence) %>% unique()
colnames(x_full) <- c("id", "sentence_id", "full_sentence")
full_new <- left_join(new, x_full, by=c("id", "sentence_id"))
full_new <- full_new %>% select(full_sentence, clusters, doc_id, sentence_id)
full_interviews_together <- full_new %>%
group_by(doc_id) %>%
summarise(text=paste(full_sentence, clusters, collapse=" "))
full_interviews_together$text[1] <- str_replace_all(full_interviews_together$text[1], "AS:", "<br> AS: ")
full_interviews_together$text[1] <- str_replace_all(full_interviews_together$text[1], "GA:", "<br> GA: ")
writeLines(full_interviews_together$text[1], "test.rmd")
View(x_full)
stopwords()
table(x$upos)
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
require(text2vec)
library(LDAvis)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)
transcripts <- readtext("All_Transcripts/*.docx")
#get rid of .docx
for(i in 1:nrow(transcripts)){
transcripts[1][i,] <- str_remove(transcripts[1][i,], ".docx")
}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")
transcripts_final <- left_join(transcripts, names, by="doc_id")
transcripts_familyexp <- transcripts_final %>% filter(family_experience==TRUE)
transcripts_treatment <- transcripts_final %>% filter(treatment_prof==TRUE)
transcripts_personal <- transcripts_final %>% filter(personal_experience==TRUE)
#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts_familyexp$text)
x <- data.frame(s)
'%nin%' <- Negate('%in%')
#take out stopwords (see stopwords() for exact words gone)
x_stopwords <- x %>%
mutate(stopword = ifelse(lemma %nin% stopwords(), "NOT.STOPWORD", "STOPWORD"))
#filter out words that aren't useful, ie. numerals, pronouns, etc.
x_filtered <- x_stopwords %>%
mutate(upos = as.character(upos)) %>%
filter(., grepl('ADJ|ADV|INTJ|NOUN|PROPN|VERB', upos)) %>% #explore: adv / intj inclusion
subset(., nchar(as.character(lemma)) >= 3) %>%
filter(., stopword != "STOPWORD")
# put words back in sentence form, but without the filtered out words
x_concatenated <- x_filtered %>%
select(doc_id, sentence_id, token, upos) %>%
group_by(sentence_id, doc_id) %>%
summarise(text=paste(token,collapse=" ")) %>%
filter(text != "Yeah") %>%
mutate(nchar=nchar(text))
colnames(x_concatenated) <- c("sentence_id", "doc_id", "sentences", "nchar")
# clean dataframe by concatenating words that appear together relatively often
# https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/
#Get skipgram probabilities
tidy_skipgrams <- x_concatenated %>%
unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>%  #sets sliding window of 2
mutate(skipgramID = row_number()) %>% #set row number as the ID
unnest_tokens(word, ngram)
#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
#find probability of each word occurring in this corpus
unigram_probs <- x_concatenated %>%
unnest_tokens(word, sentences) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
#normalize it: how often
normalized_prob <- skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2) %>%
mutate(pattern = paste(word1, word2, sep=" "), concatenate=paste(word1, word2, sep="_")) %>%
filter(word1 != word2)
normalized_prob = normalized_prob[!duplicated(normalized_prob$pattern),]
#Concatenate all words that fit the normalized probability threshold, if there are more than 10 together
if (nrow(normalized_prob)>=10){
for(i in 1:length(normalized_prob$pattern)) {
x_concatenated$sentences <- str_replace_all(x_concatenated$sentences, normalized_prob$pattern[i], normalized_prob$concatenate[i])
}
} else {
print ("No words to concatenate")
}
#add doc_id to transcripts
transcripts$id <- seq.int(nrow(transcripts))
transcripts_id <- transcripts %>% mutate(id = paste0("doc",id)) %>% select(-text)
#consistent colnames and data types
colnames(x_concatenated) <- c("sentence_id", "id", "sentence", "nchar")
#join them together
new <- left_join(x_concatenated, transcripts_id, by="id")
new <- new %>% mutate(full_id = paste(id, sentence_id, sep="_"), full_id = as.character(full_id))
require(text2vec)
set.seed(397)
tokenlen = length(new$sentence)
tokens = new$sentence[1:tokenlen] %>%
tolower %>%
word_tokenizer
it = itoken(tokens, ids = new$full_id[1:tokenlen], progressbar = FALSE)
v = create_vocabulary(it) %>%
prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
#hyperparameters
lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = tokenlen,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
#classify each sentence!!!
test <- as.data.frame(doc_topic_distr)
library(data.table)
test <- setDT(test, keep.rownames = TRUE)[]
setnames(test, 1, "full_id")
#join them together
classify <- left_join(new, test, by="full_id")
#this labels the sentence as the topic it's most likely to be from
#fix this so it has a probability threshold
classify$results <- apply(classify[7:16], 1, function(x) max(names(which(x >0))))
x_full <- x %>%
select(doc_id, sentence_id, sentence) %>%
mutate(full_id = paste(doc_id, sentence_id, sep="_"), full_id = as.character(full_id), sentence_id = as.numeric(sentence_id)) %>%
unique()
colnames(x_full) <- c("id", "sentence_id", "full_sentence", "full_id")
full_new <- left_join(classify, x_full, by=c("full_id", "sentence_id"))
full_new <- full_new %>% select(full_sentence, results, full_id, doc_id, sentence_id)
View(full_new)
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
require(text2vec)
library(LDAvis)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)
transcripts <- readtext("All_Transcripts/*.docx")
#get rid of .docx
for(i in 1:nrow(transcripts)){
transcripts[1][i,] <- str_remove(transcripts[1][i,], ".docx")
}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")
transcripts_final <- left_join(transcripts, names, by="doc_id")
#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts_final$text)
x <- data.frame(s)
x_full <- x %>%
select(doc_id, sentence_id, sentence) %>%
mutate(full_id = paste(doc_id, sentence_id, sep="_"), full_id = as.character(full_id), sentence_id = as.numeric(sentence_id)) %>%
unique()
colnames(x_full) <- c("id", "sentence_id", "full_sentence", "full_id")
full_new <- left_join(classify, x_full, by=c("full_id", "sentence_id"))
full_new <- full_new %>% select(full_sentence, results, full_id, doc_id, sentence_id)
View(full_new)
table(full_new$doc_id)
t
transcripts <- readtext("All_Transcripts/*.docx")
View(transcripts)
#get rid of .docx
for(i in 1:nrow(transcripts)){
transcripts[1][i,] <- str_remove(transcripts[1][i,], ".docx")
}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")
transcripts_final <- left_join(transcripts, names, by="doc_id")
View(transcripts_final)
#annotate model, getting words separated and sentence id where words are taken from
s <- udpipe_annotate(udmodel_english, transcripts_final$text)
x <- data.frame(s)
x_full <- x %>%
select(doc_id, sentence_id, sentence) %>%
mutate(full_id = paste(doc_id, sentence_id, sep="_"), full_id = as.character(full_id), sentence_id = as.numeric(sentence_id)) %>%
unique()
colnames(x_full) <- c("id", "sentence_id", "full_sentence", "full_id")
table(x_full$id)
View(classify)
View(transcripts)
View(transcripts_final)
View(transcripts_final)
table(x_full$id$doc_1)
table(x_full$id$doc1)
table(x_full$id[1])
table(x_full$full_sentence[1])
transcripts_final$text[1]
transcripts_final$id <- seq.int(nrow(transcripts))
View(transcripts_final)
transcripts_id <- transcripts_final %>% mutate(id = paste0("doc",id))
View(transcripts_id)
transcripts_final$id <- seq.int(nrow(transcripts))
transcripts_final <- transcripts_final %>% mutate(id = paste0("doc",id))
View(transcripts_id)
transcripts_final$id <- seq.int(nrow(transcripts))
transcripts_id <- transcripts_final %>% mutate(id = paste0("doc",id)) %>% select(doc_id, id)
x_full_test <- left_join(x_full, transcripts_id, by="doc_id")
x_full <- x %>%
select(doc_id, sentence_id, sentence) %>%
mutate(full_id = paste(doc_id, sentence_id, sep="_"), full_id = as.character(full_id), sentence_id = as.numeric(sentence_id)) %>%
unique()
colnames(x_full) <- c("doc_id", "sentence_id", "full_sentence", "full_id")
x_full_test <- left_join(x_full, transcripts_id, by="doc_id")
View(x_full_test)
transcripts_final$id <- seq.int(nrow(transcripts))
transcripts_id <- transcripts_final %>% mutate(id = paste0("doc",id)) %>% select(doc_id, id)
colnames(transcripts_id) <- c("id", "doc_id")
x_full_test <- left_join(x_full, transcripts_id, by="doc_id")
View(x_full_test)
sentences_ids <-
left_join(x_full, transcripts_id, by="doc_id") %>%
select(full_id, id, full_sentence)
View(sentences_ids)
dataset <- c("corn", "cornmeal", "corn on the cob", "meal")
# for mere occurences of the pattern:
str_count(dataset, "corn")
dataset <- c("corn", "cornmeal", "corn on the cob", "meal")
dataset <- as.data.frame(dataset)
# for mere occurences of the pattern:
str_count(dataset, "corn")
which(array(grepl("bridge",as.matrix(sentences_ids)),dim(sentences_ids)),T)
View(sentences_ids)
sentences_ids$full_sentence[5502]
which(array(grepl("under a bridge",as.matrix(sentences_ids)),dim(sentences_ids)),T)
sentences_ids$full_sentence[39267]
as.data.frame(which(array(grepl("under a bridge",as.matrix(sentences_ids)),dim(sentences_ids)),T))
word_to_find <- "under a bridge"
found_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
View(found_words)
sentence_ids[39267]
sentences_ids[39267]
sentences_ids$id[39267]
sentences_ids$sentence_id[39267]
sentences_ids$doc_id[39267]
found_words[1]
test <- found_words[1]
test
sentences_ids$full_sentence[test]
test <- found_words[1,1]
test
sentences_ids$full_sentence[test]
sentences_ids$id[test]
sentences_ids$full_sentence[test]
sentences_ids$id[test]
sentences_ids$full_id[test]
sentences_ids$full_sentence[test]
table(sentences_ids$id)
word_to_find <- "bridge"
found_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
test <- found_words[1,1]
word_to_find <- "bridge"
found_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words
found_words %>% mutate(wahoo = sentences_ids$id[row])
found_words %>%
mutate(name = sentences_ids$id[row], full_id = sentences_ids$full_id[row], sentence = sentences_ids$sentence[row])
found_words %>%
mutate(name = sentences_ids$id[row], full_id = sentences_ids$full_id[row], sentence = sentences_ids$full_sentence[row])
found_words %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row])
found_words %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
mutate(searched_for_word = word_to_find)
found_words %>%
mutate(searched_for_word = word_to_find)
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row])
found_words %>%
mutate(searched_for_word = word_to_find)
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]))
found_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row])
found_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
word_to_find <- "bridge"
find_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
write.csv(found_words, "bridge")
write.csv(found_words, "bridge.csv")
word_to_find <- "enable"
find_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
write.csv(found_words, "enable.csv")
write.csv(found_words, "enabling.csv")
word_to_find <- "enabling"
find_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
write.csv(found_words, "enabling.csv")
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
library(widyr)
require(text2vec)
library(LDAvis)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)
transcripts <- readtext("All_Transcripts/*.docx")
#get rid of .docx
for(i in 1:nrow(transcripts)){
transcripts[1][i,] <- str_remove(transcripts[1][i,], ".docx")
}
# names <- read.csv("names_descriptions.csv")
# colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")
#
# transcripts_final <- left_join(transcripts, names, by="doc_id")
# transcripts_final$id <- seq.int(nrow(transcripts))
transcripts$id <- seq.int(nrow(transcripts))
# transcripts_id <- transcripts_final %>% mutate(id = paste0("doc",id)) %>% select(doc_id, id)
# colnames(transcripts_id) <- c("id", "doc_id")
transcripts_id <- transcripts %>% mutate(id = paste0("doc",id)) %>% select(doc_id, id)
colnames(transcripts_id) <- c("id", "doc_id")
#annotate model, getting words separated and sentence id where words are taken from
# s <- udpipe_annotate(udmodel_english, transcripts_final$text)
s <- udpipe_annotate(udmodel_english, transcripts$text)
x <- data.frame(s)
x_full <- x %>%
select(doc_id, sentence_id, sentence) %>%
mutate(full_id = paste(doc_id, sentence_id, sep="_"), full_id = as.character(full_id), sentence_id = as.numeric(sentence_id)) %>%
unique()
colnames(x_full) <- c("doc_id", "sentence_id", "full_sentence", "full_id")
#Full_id has doc_id and then the sentence number
sentences_ids <-
left_join(x_full, transcripts_id, by="doc_id") %>%
select(full_id, id, full_sentence)
write.csv(sentences_ids, "sentences_ids.csv")
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
word_to_find <- "enabling"
find_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
write.csv(found_words, "enabling.csv")
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
sentences_ids <- read.csv("sentences_ids.csv")
word_to_find <- "enabling"
find_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
write.csv(found_words, "enabling.csv")
file_name <- "enabling.csv"
write.csv(found_words, file_name)
file_name <- paste(word_to_find, ".csv")
file_name
file_name <- paste(word_to_find,".csv")
?paste
file_name <- paste0(word_to_find,".csv")
file_name
word_to_find <- "write word or phrase here"
file_name <- paste0(word_to_find,".csv")
file_name
write.csv(found_words, file_name)
library(udpipe)
library(readtext)
library(corpus)
library(dplyr)
library(stopwords)
library(stringr)
library(tidytext)
sentences_ids <- read.csv("sentences_ids.csv")
word_to_find <- "test"
word_to_find <- "test"
find_words <- as.data.frame(which(array(grepl(word_to_find,as.matrix(sentences_ids)),dim(sentences_ids)),T))
found_words <- find_words %>%
mutate(searched_for_word = word_to_find) %>%
mutate(name = sentences_ids$id[row], sentence = sentences_ids$full_sentence[row], full_id = sentences_ids$full_id[row]) %>%
select(searched_for_word, name, sentence, full_id)
file_name <- paste0(word_to_find,".csv")
write.csv(found_words, file_name)
