---
title: "Clean"
author: "Fiona Adams"
date: "2/9/2020"
output: html_document
---

#Packages
```{r echo=FALSE}
library(dplyr)
library(data.table)
require(stopwords)
library(stringr)
require(text2vec)
library(readtext)
library(tidyr)
library(tidytext)
library(widyr)
```

#Read In Data
```{r}
all_transcripts <- readtext("All_Transcripts/*.docx")

for(i in 1:nrow(all_transcripts)){
  all_transcripts[1][i,] <- str_remove(all_transcripts[1][i,], " 2")
  all_transcripts[1][i,] <- str_remove(all_transcripts[1][i,], ".docx")
}

write.csv(all_transcripts, "transcripts.csv")

#nest the dataframe so each row is a document with nested text
nested_transcripts <- all_transcripts %>% nest(all_transcripts = text)

#FOR SOME REASON!! NESTED_TRANSCRIPTS IS NOT THE SAME LENGTH AS ALL_TRANSCRIPTS?? This is why this isn't working!!!

#function that splits interview into sentences
cleaninterview <- function(interview) {
  interview %>% 
    tolower() %>%
    str_replace_all("ms. ", "miss") %>%
    str_replace_all("st. ", "saint") %>%
    str_split("\\. | \\? | \\! ")
}

#make empty dataframe
interview_out <- rep(0, nrow(all_transcripts))

#apply cleaninterview function to each row of nested dataframe
for (i in 1:nrow(nested_transcripts)){
  cleandat <- cleaninterview(nested_transcripts[[2]][[i]])
  interview_out[[i]] <- cleandat
}

#turn list into dataframe with id column
interview_out <- rbindlist(interview_out, fill=FALSE, idcol = "id")
```

```{r}
names <- read.csv("names_descriptions.csv")
colnames(names) <- c("doc_id", "description", "family_experience", "treatment_prof", "personal_experience")

transcripts_final <- left_join(transcripts, names, by="doc_id")
```
Old stuff:
```{r}
transcripts <- readtext("Transcripts/*.docx")

for(i in 1:nrow(transcripts)){
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " Transcript Final copy.docx")
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " Transcript copy.docx")
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " copy.docx")
  transcripts[1][i,] <- str_remove(transcripts[1][i,], " Interview 1")
}
```


#Split into Sentences, with a doc ID
```{r}
#nest the dataframe so each row is a document with nested text
nested_transcripts <- transcripts %>% nest(transcripts = text)

#function that splits interview into sentences
cleaninterview <- function(interview) {
  interview %>% 
    tolower() %>%
    str_replace_all("ms. ", "miss") %>%
    str_replace_all("st. ", "saint") %>%
    str_split("\\. | \\? | \\! ")
}

#make empty dataframe
interview_out <- rep(0, nrow(transcripts))

#apply cleaninterview function to each row of nested dataframe
for (i in 1:nrow(nested_transcripts)){
  cleandat <- cleaninterview(nested_transcripts[[2]][[i]])
  interview_out[[i]] <- cleandat
}

#turn list into dataframe with id column
interview_out <- rbindlist(interview_out, fill=FALSE, idcol = "id")

#change column name of dataframe
colnames(interview_out)[2] <- "sentences"

#add document id

#get numerical ids for each document and take out sentences
transcripts <- transcripts %>% mutate(id = seq.int(nrow(transcripts))) %>% select(id, doc_id)

#map doc number back to the document name in dat_word using dplyr::left_join
final <- left_join(interview_out, transcripts, by="id") %>% unique()

#add sentence id

#make a table that counts how much each id occurs
count_table <- as.data.frame(table(final$id))

#initialize a list
list_out <- list()

#for the # of documents (# of ids), make a list that has the number of sentences in each doc 
for(i in 1:nrow(count_table)){
  list_out[[length(list_out)+1]] <- seq.int(count_table[2][i,])
}

#make this into a dataframe
list_out <- as.data.frame(unlist(list_out))

#combine this dataframe with final
final <- cbind(final, list_out)

#change colname to sentence id
colnames(final)[4] <- "sentence_id"
```

#Clean Data
```{r}
# fix some formatting \ns left in dataset
final$sentences <- str_replace_all(final$sentences, "\n", " ")

# take out short sentences
final <- final %>% mutate(sentences=as.character(sentences), nchar = nchar(sentences)) %>% filter(nchar>35)
#FUTURE STEP: change this so that it looks for sentences that are JUST stopwords and erases those, instead of taking out sentences of a certain length
#then, take out sentences of a certain length, but a shorter one

#remove unnecessary words (stopwords)

no.stopwords <- function(df){
  '%nin%' <- Negate('%in%')
  wordlist <- unlist(strsplit(df, " "))
  without.stopwords <- wordlist[wordlist %nin% stopwords()]
  str_c(without.stopwords,collapse=' ')  
}

#replace new$sentences[[i]] with no.stopwords(new$sentences[[i]])
for(i in 1:nrow(final)) {
  final$sentences[[i]] <- no.stopwords(final$sentences[[i]])
}

#filter out words that are just one, two, or three characters

for(i in 1:nrow(final)) {
  final$sentences[[i]] <-
paste(str_extract_all(final$sentences, '\\w{4,}')[[i]], collapse=' ')
}

# clean dataframe by concatenating words that appear together relatively often
# https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/

#Get skipgram probabilities
tidy_skipgrams <- final %>%
    unnest_tokens(ngram, sentences, token = "ngrams", n=2) %>% #sets sliding window of 2
    mutate(skipgramID = row_number()) %>% #set row number as the ID
    unnest_tokens(word, ngram)

#using counts and proportions, finds skipgram probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))

#find probability of each word occurring in this corpus
unigram_probs <- final %>%
    unnest_tokens(word, sentences) %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n))

#normalize it: how often 
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

# filters just the places where where item1 != item2 and concatenates these words
normalized_prob <- normalized_prob %>%
    dplyr::filter(word1!=word2) %>%
    mutate(
        concatenate = paste(word1, word2, sep = "_"),
        pattern = paste(word1, word2),
    )

final <- final %>%
    mutate(
        X = NULL,
        id = as.character(id),
        sentences = as.character(sentences)
    )

#How many words to concatenate? Code below only does the first 100 rows. We shouldn't do all of them, but what should be the threshold?
for (i in 1:length(normalized_prob$pattern)) {
    final$sentences <- str_replace_all(final$sentences, normalized_prob$pattern[i], normalized_prob$concatenate[i])
}
```

```{r}
write.csv(final, "clean_sentences.csv")
```

```{r}
# ex. what are the words most associated with treatment in this dataset? 
normalized_prob %>%
    filter(word1 == "treatment") %>%
    arrange(-p_together)
``` 