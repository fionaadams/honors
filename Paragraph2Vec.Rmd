---
title: "Paragraph2Vec"
author: "Fiona Adams"
date: "12/7/2019"
output: html_document
---
-not unsupervised necessarily, just not *mind-numbingly* tedious :)

-skipgram matrix! For every word, look at window before and after the word, and see how many times some other word was in the vicinity (ex. "getting")--what co-occurs? Then maybe concatenate these? Which to concatenate? (Julia Silge)
-do LDA or some other topic modeling on phrases within paragraphs, rather than words within paragraphs
-look through manually, see which ones are not helpful and could benefit from more context, then look at those rows within skipgram matrix (ex. getting, mean). Then, manually fuse them, ex getting_help. Re-run LDA on this.
-future: split the docs further, maybe 3 sentences @ a time?

```{r message=FALSE}
require(readtext)
library(dplyr)
library(data.table)
library(lattice)
library(udpipe)
library(stringr)
require(tibble)
library(stopwords)
ud_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(ud_model$file_model)

dat_word <- readtext("/Users/fionaadams/Documents/GitHub/honors/TranscriptTest/*.docx")
```

```{r}
cleaninterview <- function(interview) {
  interview %>% 
    tolower() %>%
    str_replace_all("ms.", "miss") %>%
    strsplit("\\n+") %>%
    as.data.frame()
}

changecolname <- function(interview) {
  colnames(interview)[1] <- "paragraphs"
  interview$paragraphs <- as.character(interview$paragraphs)
  interview
}

allparagraphs <- function(interviewfolder){
  interview_out <- rep(0, nrow(dat_word))
  
  for (i in 1:nrow(dat_word)) {
    cleandat <- changecolname(cleaninterview(dat_word$text[i]))
    interview_out[[i]] <- cleandat
  }
  
  paragraphs <- rbindlist(interview_out, fill=TRUE)
  return(paragraphs) #this puts them all into one data table
}

new <- allparagraphs(dat_word)

new <- new %>% 
  rowid_to_column("ID") %>% #make ID column
  mutate(ID = paste("doc", ID), paragraphs=as.character(paragraphs), nchar = nchar(paragraphs)) %>%
  filter(grepl("as:", paragraphs)==FALSE, nchar>20) #take out Amy Sullivan's questions 
```

Clean dataframe by taking out unimportant words
```{r}
no.stopwords <- function(df){
  '%nin%' <- Negate('%in%')
  wordlist <- unlist(strsplit(df, " "))
  without.stopwords <- wordlist[wordlist %nin% stopwords()]
  str_c(without.stopwords,collapse=' ')  
}

#test: show what this does
new$paragraphs[[1]]
no.stopwords(new$paragraphs[[1]])

#replace new$paragraphs[[i]] with no.stopwords(new$paragraphs[[i]])
for(i in 1:nrow(new)) {
  new$paragraphs[[i]] <- no.stopwords(new$paragraphs[[i]])
}
```

Make a .csv file for using in future
```{r}
write.csv(new,'cleanparagraphs.csv')
```

Find top noun/verb phrases, filter them out
```{r}
#finds parts of speech in entire corpus
x <- udpipe_annotate(udmodel_english, x = new$paragraphs, trace = TRUE)
x <- as.data.frame(x)

#takes top 100 nouns from entire corpus
nouns <- subset(x, upos %in% "NOUN")
nouns <- txt_freq(x = nouns$lemma)
nouns$key <- factor(nouns$key, levels = rev(nouns$key))
top25nouns <- head(nouns$key, 100)

#takes top 100 nouns from entire corpus
adjs <- subset(x, upos %in% "ADJ")
adjs <- txt_freq(x = adjs$lemma)
adjs$key <- factor(adjs$key, levels = rev(adjs$key))
top25adjs <- head(adjs$key, 100)

#shows top 25 nouns and adjectives
barchart(key ~ freq, data = head(nouns, 25), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
barchart(key ~ freq, data = head(adjs, 25), col = "cadetblue", main = "Most occurring adjectives", xlab = "Freq")
```

```{r}
only.nouns <- function(df){
  wordlist <- unlist(strsplit(df, " "))
  without.stopwords <- wordlist[wordlist %in% top25nouns]
  str_c(without.stopwords,collapse=' ')  
}

#test: show what this does
new$paragraphs[[1]] 
only.nouns(new$paragraphs[[1]])

#replace new$paragraphs[[i]] with only.nouns(new$paragraphs[[i]])
# for(i in 1:nrow(new)) {
#   new$paragraphs[[i]] <- only.nouns(new$paragraphs[[i]])
# }
```

LDA
```{r}
require(text2vec)
tokens = new$paragraphs[1:1000] %>% 
  tolower %>%
  word_tokenizer

it = itoken(tokens, ids = new$ID[1:1000], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

```{r}
barplot(doc_topic_distr[1, ], xlab = "topic", 
        ylab = "proportion", ylim = c(0, 1), 
        names.arg = 1:ncol(doc_topic_distr))
```

```{r}
new_dtm = itoken(new$paragraphs[1000:2116], tolower, word_tokenizer, ids = new$ID[1000:2116]) %>% 
  create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
```

```{r}
library(LDAvis)
lda_model$plot()
```

Different way to LDA
```{r}
require(quanteda)
require(tm)
require(tidytext)
#Make simple corpus
corpus <- Corpus(VectorSource(new$paragraphs)) 

#Clean simple corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)

#Turn into complex corpus
corpus <- corpus(corpus)
```

```{r}
corpusdfm <- quanteda::dfm(corpus, verbose = FALSE)

tidycorpus <- tidy(corpusdfm)

dtm <- tidycorpus %>% cast_dtm(document, term, count)
```

```{r}
library(topicmodels)

# set a seed so that the output of the model is predictable
ap_lda <- LDA(dtm, k = 10, control = list(seed = 1234))
ap_lda

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```

```{r}
library(ggplot2)
library(dplyr)
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

*How to label each # of rows for the name of the doc_id?*
https://stackoverflow.com/questions/28648513/changing-column-names-in-a-list-of-data-frames-in-r
```{r}
colnames = as.list(dat_word$doc_id) #gives us the doc_ids as a list
dfList <- new
lapply(dfList, setNames, colnames) #tries to apply colnames to dfList, but not working
```



